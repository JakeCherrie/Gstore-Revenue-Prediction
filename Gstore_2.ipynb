{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Google Analytics Customer Revenue Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File system manangement\n",
    "import os\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 300)\n",
    "# Importing random for random selections\n",
    "import random\n",
    "# Json for importing JSON columns\n",
    "import json as json\n",
    "# Pandas io json normalizing\n",
    "from pandas.io.json import json_normalize\n",
    "# Scipy stats for statistical analysis\n",
    "import scipy.stats as stats\n",
    "# sklearn preprocessing for dealing with categorical features\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# to use for woe binning for features with a large number of categories\n",
    "import scorecardpy as sc\n",
    "# Light gradient boost classifier\n",
    "from lightgbm import LGBMRegressor\n",
    "# Sklearn Inputing data spliting method\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "# Sklearn importing auc as measurement metric\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Gc memory managment\n",
    "import gc\n",
    "# Matplotlib pyplot for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# Seabourne for visualization\n",
    "import seaborn as sns\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up package to import data converting JSON columns into individual features\n",
    "# https://www.kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields\n",
    "JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "def load_df(csv_path):\n",
    "    df = pd.read_csv(csv_path, converters={column: json.loads for column in JSON_COLUMNS}\\\n",
    "                     , dtype={'fullVisitorId': 'str', 'visitStartTime': 'str', 'date': 'str'})\n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeting the input and output directory\n",
    "os.chdir('C:/Users/Jake Cherrie/Documents/Projects/Gstore-Revenue-Prediction')\n",
    "# Viewing the contained datasets\n",
    "os.listdir('C:/Users/Jake Cherrie/Data Sets/Gstore Revenue Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the training data\n",
    "trn_df = load_df('C:/Users/Jake Cherrie/Data Sets/Gstore Revenue Prediction/train.csv')\n",
    "trn_df['totals.transactionRevenue'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick view of training data\n",
    "trn_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping features with no information\n",
    "drp_cols = [col for col in trn_df.columns if trn_df[col].nunique() == 1 & trn_df[col].notnull().values.all()]\n",
    "trn_df = trn_df.drop(columns=drp_cols)\n",
    "# Dropping duplicate feature visitStartTime is the same as visitId\n",
    "trn_df = trn_df.drop(columns='visitId')\n",
    "# Dropping capaign code as there is ony 1 non-null entry\n",
    "trn_df = trn_df.drop(columns='trafficSource.campaignCode')\n",
    "# Noticed that for the training set channelGrouping contains all the information in trafficSource.medium so droping trafficSource.medium\n",
    "trn_df = trn_df.drop(columns='trafficSource.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving memory by shortning ints and floats\n",
    "def size_reduction(df):\n",
    "    int_col     = df.select_dtypes(include=[np.int64]).columns\n",
    "    flt_col     = df.select_dtypes(include=[np.float64]).columns \n",
    "    df[int_col] = df[int_col].astype(np.int32)\n",
    "    df[flt_col] = df[flt_col].astype(np.float32)\n",
    "    \n",
    "size_reduction(trn_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the testing data\n",
    "tst_df = load_df('C:/Users/Jake Cherrie/Data Sets/Gstore Revenue Prediction/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick view of training data\n",
    "tst_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping features with no information\n",
    "drp_cols = [col for col in tst_df.columns if tst_df[col].nunique() == 1 & tst_df[col].notnull().values.all()]\n",
    "tst_df = tst_df.drop(columns=drp_cols)\n",
    "# Dropping duplicate feature visitStartTime is the same as visitId\n",
    "tst_df = tst_df.drop(columns='visitId')\n",
    "tst_df = tst_df.drop(columns='trafficSource.medium')\n",
    "\n",
    "size_reduction(tst_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data for Quick Loads and Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df.to_csv('C:/Users/Jake Cherrie/Data Sets/Gstore Revenue Prediction/trn.csv', index=False)\n",
    "tst_df.to_csv('C:/Users/Jake Cherrie/Data Sets/Gstore Revenue Prediction/tst.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seting up debugging sets for quicker exploration\n",
    "debugTrn = sorted(random.sample(range(1,903653),903653-200000))\n",
    "debugTst = sorted(random.sample(range(1,804684),804684-100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df = pd.read_csv('C:/Users/Jake Cherrie/Data Sets/Gstore Revenue Prediction/trn.csv'\\\n",
    "           , dtype={'fullVisitorId': 'str', 'visitStartTime': 'str', 'date': 'str'}, skiprows=None, nrows=None)\n",
    "tst_df = pd.read_csv('C:/Users/Jake Cherrie/Data Sets/Gstore Revenue Prediction/tst.csv'\\\n",
    "           , dtype={'fullVisitorId': 'str', 'visitStartTime': 'str', 'date': 'str'}, skiprows=None, nrows=None)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df.set_index('fullVisitorId', inplace=True)\n",
    "tst_df.set_index('fullVisitorId', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Combination Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a has revenue flag\n",
    "trn_df['hasRevenue'] = (trn_df['totals.transactionRevenue'] > 0).astype(int)\n",
    "trn_df['logRevenue'] = np.log1p(trn_df['totals.transactionRevenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at first and last shows that it is clearly a time dependent problem\n",
    "trn_df['visitStartTime'] = pd.to_datetime(trn_df['visitStartTime'],unit='s')\n",
    "print(trn_df['visitStartTime'].describe())\n",
    "tst_df['visitStartTime'] = pd.to_datetime(tst_df['visitStartTime'],unit='s')\n",
    "print(tst_df['visitStartTime'].describe())\n",
    "# should explore weighting the more ecent data lgb(weight=w)\n",
    "trn_df['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df['weekday'] = trn_df['visitStartTime'].dt.weekday.astype(str)\n",
    "tst_df['weekday'] = tst_df['visitStartTime'].dt.weekday.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df['time']  = trn_df['visitStartTime'].dt.hour.astype(str)\n",
    "tst_df['time']  = tst_df['visitStartTime'].dt.hour.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the weekday relationship to the log1p revenue\n",
    "sns.barplot('weekday', 'logRevenue', data = trn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting count per group\n",
    "sns.countplot('weekday', data=trn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight of Evidence (WOE) encoding\n",
    "weekday = trn_df[['weekday','hasRevenue']]\n",
    "#bins_weekday = sc.woebin(weekday, y='hasRevenue', stop_limit=0.02, max_num_bin=2, method='tree')\n",
    "#Saving for quick load    \n",
    "#np.save('C:/Users/Jake Cherrie/Data Sets/Gstore Revenue Prediction/Bins sets/bins_weekday.npy', bins_weekday) \n",
    "# Quick load of dataframe\n",
    "bins_weekday = np.load('C:/Users/Jake Cherrie/Data Sets/Gstore Revenue Prediction/Bins sets/bins_weekday.npy').item()\n",
    "bins_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying WOE encoding\n",
    "trn_df['weekday'] = sc.woebin_ply(trn_df, bins_weekday)['weekday_woe']\n",
    "tst_df['weekday'] = sc.woebin_ply(tst_df, bins_weekday)['weekday_woe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unused features\n",
    "trn_set = trn_df.drop(columns=['visitStartTime', 'date', 'sessionId'\\\n",
    "                                , 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.keyword'])\n",
    "tst_set = tst_df.drop(columns=['visitStartTime', 'date', 'sessionId'\\\n",
    "                                , 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating feature and target sets\n",
    "trn_fts = trn_set[tst_set.columns]\n",
    "trn_tgt = trn_set['logRevenue']\n",
    "tst_fts = tst_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up categorical columns\n",
    "cat_col = [col for col in trn_fts.columns if trn_fts[col].dtype == 'object']\n",
    "# factorizing categorical columns\n",
    "for col in cat_col:\n",
    "    indexer = pd.factorize(trn_set[col])[1]\n",
    "    trn_fts[col] = indexer.get_indexer(trn_fts[col])\n",
    "    tst_fts[col] = indexer.get_indexer(tst_fts[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visit ID Level Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing lgb paramaters\n",
    "params={'num_leaves': 31,\n",
    "        'max_depth': 15,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 1000,\n",
    "        'metric':'rmse',\n",
    "        'num_leaves': 31,\n",
    "        'verbose': 1,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"random_state\":42,\n",
    "        'min_child_samples': 20\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_vis = np.array(sorted(pd.Series(trn_df.index.values).unique()))\n",
    "# Get folds\n",
    "Kflds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "flds = []\n",
    "for trn, vld in Kflds.split(X=unq_vis, y=unq_vis):\n",
    "    flds.append([unq_vis[trn],unq_vis[vld]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Variables\n",
    "trn_df['prd'] = 0\n",
    "trn_prd =  trn_df['prd']\n",
    "tst_prd = np.zeros(tst_fts.shape[0])\n",
    "imp = pd.DataFrame()\n",
    "wgt_sum = 0\n",
    "MSE = 0\n",
    "\n",
    "for n_fld ,(trn_idx, vld_idx) in enumerate(flds):\n",
    "    #trn_idx = idx_tab.iloc[trn_num].index\n",
    "    #vld_idx = idx_tab.iloc[vld_num].index\n",
    "    trn_X, trn_y = trn_fts.loc[trn_idx], trn_tgt.loc[trn_idx]\n",
    "    vld_X, vld_y = trn_fts.loc[vld_idx], trn_tgt.loc[vld_idx]\n",
    "    \n",
    "    lgb = LGBMRegressor(**params)\n",
    "    \n",
    "    # Fit the model\n",
    "    lgb.fit(trn_X, trn_y)\n",
    "    \n",
    "    # applying the model to the validation data\n",
    "    val_prd = lgb.predict(vld_X)\n",
    "    val_prd[val_prd < 0] = 0\n",
    "    # Calculating and outputting the RMSE\n",
    "    fld_MSE = mean_squared_error(vld_y, val_prd)\n",
    "    print('Fold %2d RMSE : %.6f' % (n_fld + 1, np.sqrt(fld_MSE)))\n",
    "    \n",
    "    # Summing mean squared errors\n",
    "    MSE += fld_MSE/5\n",
    "    wgt_sum += 1/np.sqrt(fld_MSE)\n",
    "    \n",
    "    # Applying predictions to the train set weighted by the MSE\n",
    "    prd = lgb.predict(vld_X)\n",
    "    prd[prd < 0] = 0\n",
    "    trn_prd.loc[vld_idx] = prd\n",
    "    \n",
    "    # Applying predictions to the test set weighted by the MSE\n",
    "    prd = lgb.predict(tst_fts)\n",
    "    prd[prd < 0] = 0\n",
    "    tst_prd += prd/np.sqrt(fld_MSE)\n",
    "    \n",
    "    # Calculating the fold importance\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['fts'] = trn_X.columns\n",
    "    imp_df['imp'] = lgb.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    # Summing the fold importances\n",
    "    imp_df['fld'] = n_fld+1\n",
    "    imp = pd.concat([imp, imp_df], axis=0, sort=False)\n",
    "\n",
    "# Scaling the predictions\n",
    "trn_fts['prd'] = np.expm1(trn_prd)\n",
    "trn_fts['log_prd'] = trn_prd\n",
    "tst_fts['prd'] = np.expm1(tst_prd/wgt_sum)\n",
    "tst_fts['log_prd'] = tst_prd/wgt_sum\n",
    "#1.632441\n",
    "np.sqrt(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting stage 1 feature importances\n",
    "cols = imp[[\"fts\", \"imp\"]].groupby(\"fts\").mean().sort_values(by=\"imp\", ascending=False)[:60].index\n",
    "imp['log1p_imp'] = np.log1p(imp['imp'])\n",
    "best_features = imp.loc[imp.fts.isin(cols)]\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.barplot(x=\"log1p_imp\", y=\"fts\", data=best_features.sort_values(by=\"imp\", ascending=False))\n",
    "plt.title('Features (avg over folds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: Perform aggregations\n",
    "aggregations = {\n",
    "    'channelGrouping': ['median'],\n",
    "    'visitNumber': ['max', 'sum'],\n",
    "    'device.browser': ['median'],\n",
    "    'device.deviceCategory': ['median'], \n",
    "    'device.isMobile': ['max', 'sum', 'min'], \n",
    "    'device.operatingSystem': ['median'],\n",
    "    'geoNetwork.city': ['median'], \n",
    "    'geoNetwork.continent': ['median'], \n",
    "    'geoNetwork.country': ['median'],\n",
    "    'geoNetwork.metro': ['median'], \n",
    "    'geoNetwork.networkDomain': ['median'], \n",
    "    'geoNetwork.region': ['median'],\n",
    "    'geoNetwork.subContinent': ['median'], \n",
    "    'totals.bounces': ['max','min','sum'], \n",
    "    'totals.hits': ['max'],\n",
    "    'totals.newVisits': ['max'], \n",
    "    'totals.pageviews': ['max'], \n",
    "    'trafficSource.adContent': ['median'],\n",
    "    'trafficSource.adwordsClickInfo.adNetworkType': ['median'],\n",
    "    #'trafficSource.adwordsClickInfo.gclId': ['median'],\n",
    "    'trafficSource.adwordsClickInfo.isVideoAd': ['median'],\n",
    "    'trafficSource.adwordsClickInfo.page': ['median'],\n",
    "    'trafficSource.adwordsClickInfo.slot': ['median'], \n",
    "    'trafficSource.campaign': ['median'],\n",
    "    'trafficSource.isTrueDirect': ['median'], \n",
    "    #'trafficSource.keyword': ['median'],\n",
    "    'trafficSource.referralPath': ['median'], \n",
    "    'trafficSource.source': ['median'],\n",
    "    'log_prd': ['sum','mean','max'],\n",
    "    'prd': ['sum','mean','max']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_agg = trn_fts.groupby('fullVisitorId').agg(aggregations)\n",
    "trn_agg.columns = pd.Index([e[0] + \".\" + e[1].upper() for e in trn_agg.columns.tolist()])\n",
    "tst_agg = tst_fts.groupby('fullVisitorId').agg(aggregations)\n",
    "tst_agg.columns = pd.Index([e[0] + \".\" + e[1].upper() for e in tst_agg.columns.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_agg['prd.SUM'] = np.log1p(trn_agg['prd.SUM'])\n",
    "tst_agg['prd.SUM'] = np.log1p(tst_agg['prd.SUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_agg = np.log1p(trn_df.groupby('fullVisitorId').sum()['totals.transactionRevenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_agg.shape, tst_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing stage 2 paramaters\n",
    "params={'num_leaves': 31,\n",
    "        'max_depth': 15,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_estimators': 1000,\n",
    "        'num_leaves': 31,\n",
    "        'verbose': 100,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"random_state\":42,\n",
    "        'lambda_l2': 0.02085548700474218,\n",
    "        'lambda_l1': 0.004107624022751344,\n",
    "        'bagging_fraction': 0.7934712636944741,\n",
    "        'feature_fraction': 0.686612409641711\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Variables\n",
    "sub_prd = np.zeros(tst_agg.shape[0])\n",
    "imp = pd.DataFrame()\n",
    "wgt_sum  = 0\n",
    "MSE = 0\n",
    "\n",
    "for n_fld, (trn_idx, vld_idx) in enumerate(flds):\n",
    "    #trn_idx = idx_tab.iloc[trn_num].index\n",
    "    #vld_idx = idx_tab.iloc[vld_num].index\n",
    "    trn_X, trn_y = trn_agg.loc[trn_idx], tgt_agg.loc[trn_idx]\n",
    "    vld_X, vld_y = trn_agg.loc[vld_idx], tgt_agg.loc[vld_idx]\n",
    "\n",
    "    lgb = LGBMRegressor(**params)\n",
    "    \n",
    "    # Fit the model\n",
    "    lgb.fit(trn_X, trn_y)\n",
    "    \n",
    "    # applying the model to the validation data\n",
    "    vld_prd = lgb.predict(vld_X)\n",
    "    vld_prd[vld_prd < 0] = 0\n",
    "    # Calculating and outputting the RMSE\n",
    "    fld_MSE = mean_squared_error(vld_y, vld_prd)\n",
    "    print('Fold %2d RMSE : %.6f' % (n_fld + 1, np.sqrt(fld_MSE)))\n",
    "    \n",
    "    # Summing mean squared errors\n",
    "    MSE += fld_MSE/5\n",
    "    wgt_sum += 1/np.sqrt(fld_MSE) \n",
    "    \n",
    "    # Applying predictions\n",
    "    prd = lgb.predict(tst_agg)\n",
    "    prd[prd < 0] = 0\n",
    "    sub_prd += prd*wgt_sum\n",
    "    \n",
    "    # Calculating the fold importance\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['fts'] = trn_X.columns\n",
    "    imp_df['imp'] = lgb.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    # Summing the fold importances\n",
    "    imp_df['fld'] = n_fld+1\n",
    "    imp = pd.concat([imp, imp_df], axis=0, sort=False)\n",
    "\n",
    "np.sqrt(MSE)\n",
    "# ~LB = 1.4412\n",
    "# 1.6064791269377698 = 1.4412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting stage 2 feature importances\n",
    "cols = imp[[\"fts\", \"imp\"]].groupby(\"fts\").mean().sort_values(by=\"imp\", ascending=False)[:80].index\n",
    "imp['log1p_imp'] = np.log1p(imp['imp'])\n",
    "best_features = imp.loc[imp.fts.isin(cols)]\n",
    "plt.figure(figsize=(8, 14))\n",
    "sns.barplot(x=\"log1p_imp\", y=\"fts\", data=best_features.sort_values(by=\"imp\", ascending=False))\n",
    "plt.title('Features (avg over folds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_agg['predictedLogRevenue'] = sub_prd\n",
    "tst_agg['predictedLogRevenue'].to_csv('submission.csv', header = True, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
